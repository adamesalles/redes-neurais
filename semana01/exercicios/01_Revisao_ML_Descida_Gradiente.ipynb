{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "## Revisão de Aprendizado de Máquina e Exemplo de Descida de Gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "Neste notebook, resolveremos um problema simples de regressão linear usando descida de gradiente.  \n",
    "Veremos o efeito da taxa de aprendizagem na trajetória no espaço de parâmetros.\n",
    "Mostraremos como a Descida de Gradiente Estocástica (SGD) difere da versão padrão, e o efeito de \"embaralhar\" seus dados durante o SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparação inicial - pacotes a serem carregados\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Gerar Dados de uma Distribuição Conhecida\n",
    "Abaixo geraremos dados de uma distribuição conhecida.  \n",
    "Especificamente, o modelo verdadeiro é:\n",
    "\n",
    "$Y = b + \\theta_1 X_1 + \\theta_2 X_2 + \\epsilon$\n",
    "\n",
    "$X_1$ e $X_2$ têm uma distribuição uniforme no intervalo $[0,10]$, enquanto `const` é um vetor de uns (representando o termo de intercepto).\n",
    "\n",
    "Definimos valores reais para $b$, $\\theta_1$, e $\\theta_2$\n",
    "\n",
    "Aqui $b=1.5$, $\\theta_1=2$, e $\\theta_2=5$\n",
    "\n",
    "Em seguida, geramos um vetor de valores de $y$ de acordo com o modelo e juntamos os preditores em uma \"matriz de características\" `x_mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)  ## Isso garante que obtemos os mesmos dados se todos os outros parâmetros permanecerem fixos\n",
    "\n",
    "num_obs = 100\n",
    "x1 = np.random.uniform(0,10,num_obs)\n",
    "x2 = np.random.uniform(0,10,num_obs)\n",
    "const = np.ones(num_obs)\n",
    "eps = np.random.normal(0,.5,num_obs)\n",
    "\n",
    "b = 1.5\n",
    "theta_1 = 2\n",
    "theta_2 = 5\n",
    "\n",
    "y = b*const+ theta_1*x1 + theta_2*x2 + eps\n",
    "\n",
    "x_mat = np.array([const,x1,x2]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Obter a Resposta \"Correta\" Diretamente\n",
    "Nas células abaixo, resolvemos para o conjunto ótimo de coeficientes. Note que, embora o modelo verdadeiro seja dado por:\n",
    "\n",
    "$b=1.5$, $\\theta_1=2$, e $\\theta_2=5$\n",
    "\n",
    "A estimativa de máxima verossimilhança (mínimos quadrados) de um conjunto de dados finito pode ser ligeiramente diferente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Exercício:\n",
    "Resolva o problema de duas maneiras: \n",
    "1. Usando o modelo LinearRegression do scikit-learn\n",
    "2. Usando álgebra matricial diretamente através da fórmula $\\theta = (X^T X)^{-1}X^Ty$\n",
    "\n",
    "Nota: O solver do scikit-learn pode mostrar uma mensagem de aviso, isso pode ser ignorado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Resolver diretamente usando sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_model = LinearRegression(fit_intercept=False)\n",
    "lr_model.fit(x_mat, y)\n",
    "\n",
    "lr_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resolver por cálculo matricial\n",
    "np.linalg.inv(np.dot(x_mat.T,x_mat)).dot(x_mat.T).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Resolvendo por Descida de Gradiente\n",
    "Outra maneira de resolver este problema é usar o método de Descida de Gradiente. Exploraremos este método porque (como veremos) as Redes Neurais são treinadas por Descida de Gradiente. Ver como a descida de gradiente funciona em um exemplo simples construirá intuição e nos ajudará a entender algumas das nuances sobre definir a taxa de aprendizagem. Também exploraremos a Descida de Gradiente Estocástica e compararemos seu comportamento com a abordagem padrão."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Exercício\n",
    "\n",
    "As próximas várias células contêm código para executar descida de gradiente (full-batch). Omitimos alguns parâmetros para você preencher.\n",
    "\n",
    "1. Escolha uma taxa de aprendizagem e um número de iterações, execute o código e então plote a trajetória de sua descida de gradiente.\n",
    "1. Encontre exemplos onde a taxa de aprendizagem é muito alta, muito baixa e \"perfeita\".\n",
    "1. Observe os gráficos da função de perda sob essas condições.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parâmetros para experimentar\n",
    "learning_rate = .00001\n",
    "num_iter = 1000\n",
    "theta_initial = np.array([3,3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Passos de inicialização\n",
    "theta = theta_initial\n",
    "theta_path = np.zeros((num_iter+1,3))\n",
    "theta_path[0,:]= theta_initial\n",
    "\n",
    "loss_vec = np.zeros(num_iter)\n",
    "\n",
    "## Loop principal da Descida de Gradiente (para um número fixo de iterações)\n",
    "for i in range(num_iter):\n",
    "    y_pred = np.dot(theta.T,x_mat.T)\n",
    "    loss_vec[i] = np.sum((y-y_pred)**2)\n",
    "    grad_vec = (y-y_pred).dot(x_mat)/num_obs  #soma os gradientes de todas as observações e divide por num_obs\n",
    "    grad_vec = grad_vec\n",
    "    theta = theta + learning_rate*grad_vec\n",
    "    theta_path[i+1,:]=theta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "## Plotar os resultados - é um espaço de parâmetros 3d - plotamos fatias 2d\n",
    "## Amarelo é ponto inicial e azul é ponto final\n",
    "plt.figure(figsize = (30,20))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(theta_path[:,1],theta_path[:,2],'k-x')\n",
    "plt.plot(theta_path[0,1],theta_path[0,2],'yo')\n",
    "plt.plot(theta_path[-1,1],theta_path[-1,2],'bo')\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(theta_path[:,0],theta_path[:,1],'k-x')\n",
    "plt.plot(theta_path[0,0],theta_path[0,1],'yo')\n",
    "plt.plot(theta_path[-1,0],theta_path[-1,1],'bo')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(theta_path[:,0],theta_path[:,2],'k-x')\n",
    "plt.plot(theta_path[0,0],theta_path[0,2],'yo')\n",
    "plt.plot(theta_path[-1,0],theta_path[-1,2],'bo')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(loss_vec)\n",
    "plt.ylim([0,500])\n",
    "\n",
    "## Plotar a função de perda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Descida de Gradiente Estocástica\n",
    "Em vez de calcular a média dos gradientes em todo o conjunto de dados antes de dar um passo, agora daremos um passo para cada ponto de dados. Cada passo será uma \"reação excessiva\", mas eles devem se equilibrar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Exercício\n",
    "O código abaixo executa a Descida de Gradiente Estocástica, mas percorre os dados na mesma ordem a cada vez.  \n",
    "\n",
    "1. Execute o código e plote os gráficos. O que você observa?\n",
    "2. Modifique o código para que ele reordene os dados aleatoriamente. Como as trajetórias das amostras se comparam? _VOCÊ DEVE COMPLETAR AQUI_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parâmetros para experimentar\n",
    "learning_rate = .002\n",
    "num_iter = 10 #O número de \"passos\" será num_iter * numobs\n",
    "theta_initial = np.array([3,3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Passos de inicialização\n",
    "theta = theta_initial\n",
    "theta_path = np.zeros(((num_iter*num_obs)+1,3))\n",
    "theta_path[0,:]= theta_initial\n",
    "loss_vec = np.zeros(num_iter*num_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop principal do SGD\n",
    "count = 0\n",
    "for i in range(num_iter):\n",
    "    for j in range(num_obs):\n",
    "        count+=1\n",
    "        y_pred = np.dot(theta.T,x_mat.T)\n",
    "        loss_vec[count-1] = np.sum((y-y_pred)**2)\n",
    "        grad_vec = (y[j]-y_pred[j])*(x_mat[j,:])\n",
    "        theta = theta + learning_rate*grad_vec\n",
    "        theta_path[count,:]=theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotar os resultados - é um espaço de parâmetros 3d - plotamos fatias 2d\n",
    "## Amarelo é ponto inicial e azul é ponto final\n",
    "plt.figure(figsize = (30,20))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(theta_path[:,1],theta_path[:,2],'k-x')\n",
    "plt.plot(theta_path[0,1],theta_path[0,2],'yo')\n",
    "plt.plot(theta_path[-1,1],theta_path[-1,2],'bo')\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(theta_path[:,0],theta_path[:,1],'k-x')\n",
    "plt.plot(theta_path[0,0],theta_path[0,1],'yo')\n",
    "plt.plot(theta_path[-1,0],theta_path[-1,1],'bo')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(theta_path[:,0],theta_path[:,2],'k-x')\n",
    "plt.plot(theta_path[0,0],theta_path[0,2],'yo')\n",
    "plt.plot(theta_path[-1,0],theta_path[-1,2],'bo')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(loss_vec)\n",
    "plt.ylim([0,500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Agora é com você!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
