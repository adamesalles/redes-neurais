def stochastic_gradient_descent(X, y, learning_rate=0.01, max_epochs=100, shuffle=True):
    """
    Implementa descida de gradiente estocástica
    """
    n, p = X.shape
    beta = np.random.normal(0, 1, p)
    cost_history = []
    beta_history = [beta.copy()]

    for epoch in range(max_epochs):
        # Embaralhar dados
        if shuffle:
            indices = np.random.permutation(n)
            X_shuffled = X[indices]
            y_shuffled = y[indices]
        else:
            X_shuffled = X
            y_shuffled = y

        # Iterar sobre cada amostra
        for i in range(n):
            xi = X_shuffled[i:i+1]  # Manter dimensão (1, p)
            yi = y_shuffled[i]

            # Predição e erro para amostra i
            y_pred_i = xi @ beta
            error_i = yi - y_pred_i[0]

            # Atualização usando gradiente da amostra i
            gradient_i = -xi.T @ np.array([error_i])
            beta = beta - learning_rate * gradient_i.flatten()

            # Salvar histórico
            beta_history.append(beta.copy())

        # Calcular custo total após época
        y_pred_all = X @ beta
        cost = 0.5 * np.mean((y - y_pred_all)**2)
        cost_history.append(cost)

    return beta, cost_history, np.array(beta_history)
