\begin{MintedVerbatim}[commandchars=\\\{\}]
\PYG{k}{def}\PYG{+w}{ }\PYG{n+nf}{gradient\PYGZus{}descent}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{,} \PYG{n}{max\PYGZus{}iterations}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{n}{tolerance}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}6}\PYG{p}{):}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Implementa descida de gradiente para regressão linear}

\PYG{l+s+sd}{    Parâmetros:}
\PYG{l+s+sd}{    \PYGZhy{} X: matriz de design (n x p)}
\PYG{l+s+sd}{    \PYGZhy{} y: vetor de variáveis dependentes (n,)}
\PYG{l+s+sd}{    \PYGZhy{} learning\PYGZus{}rate: taxa de aprendizado}
\PYG{l+s+sd}{    \PYGZhy{} max\PYGZus{}iterations: número máximo de iterações}
\PYG{l+s+sd}{    \PYGZhy{} tolerance: critério de convergência}

\PYG{l+s+sd}{    Retorna:}
\PYG{l+s+sd}{    \PYGZhy{} beta: coeficientes estimados}
\PYG{l+s+sd}{    \PYGZhy{} cost\PYGZus{}history: histórico da função de custo}
\PYG{l+s+sd}{    \PYGZhy{} beta\PYGZus{}history: histórico dos parâmetros}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{n}\PYG{p}{,} \PYG{n}{p} \PYG{o}{=} \PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}

    \PYG{c+c1}{\PYGZsh{} Inicialização}
    \PYG{n}{beta} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{p}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} ou np.zeros(p)}
    \PYG{n}{cost\PYGZus{}history} \PYG{o}{=} \PYG{p}{[]}
    \PYG{n}{beta\PYGZus{}history} \PYG{o}{=} \PYG{p}{[}\PYG{n}{beta}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{()]}

    \PYG{k}{for} \PYG{n}{iteration} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{max\PYGZus{}iterations}\PYG{p}{):}
        \PYG{c+c1}{\PYGZsh{} Predições}
        \PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{X} \PYG{o}{@} \PYG{n}{beta}

        \PYG{c+c1}{\PYGZsh{} Custo}
        \PYG{n}{cost} \PYG{o}{=} \PYG{l+m+mf}{0.5} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{((}\PYG{n}{y} \PYG{o}{\PYGZhy{}} \PYG{n}{y\PYGZus{}pred}\PYG{p}{)}\PYG{o}{**}\PYG{l+m+mi}{2}\PYG{p}{)}
        \PYG{n}{cost\PYGZus{}history}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{cost}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Gradiente}
        \PYG{n}{gradient} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{n}\PYG{p}{)} \PYG{o}{*} \PYG{n}{X}\PYG{o}{.}\PYG{n}{T} \PYG{o}{@} \PYG{p}{(}\PYG{n}{X} \PYG{o}{@} \PYG{n}{beta} \PYG{o}{\PYGZhy{}} \PYG{n}{y}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Atualização}
        \PYG{n}{beta\PYGZus{}new} \PYG{o}{=} \PYG{n}{beta} \PYG{o}{\PYGZhy{}} \PYG{n}{learning\PYGZus{}rate} \PYG{o}{*} \PYG{n}{gradient}

        \PYG{c+c1}{\PYGZsh{} Verificar convergência}
        \PYG{k}{if} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(}\PYG{n}{beta\PYGZus{}new} \PYG{o}{\PYGZhy{}} \PYG{n}{beta}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n}{tolerance}\PYG{p}{:}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}Convergiu em }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{iteration}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{ iterações\PYGZdq{}}\PYG{p}{)}
            \PYG{k}{break}

        \PYG{n}{beta} \PYG{o}{=} \PYG{n}{beta\PYGZus{}new}
        \PYG{n}{beta\PYGZus{}history}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{beta}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{())}

    \PYG{k}{return} \PYG{n}{beta}\PYG{p}{,} \PYG{n}{cost\PYGZus{}history}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{beta\PYGZus{}history}\PYG{p}{)}
\end{MintedVerbatim}
