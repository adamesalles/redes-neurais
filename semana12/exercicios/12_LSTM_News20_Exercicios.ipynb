{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício: Usando LSTMs para Classificar o Conjunto de Dados 20 Newsgroups\n",
    "O conjunto de dados 20 Newsgroups é um problema de classificação bem conhecido. O objetivo é classificar de qual grupo de notícias veio uma postagem específica. Os 20 grupos possíveis são:\n",
    "\n",
    "`comp.graphics\n",
    "comp.os.ms-windows.misc\n",
    "comp.sys.ibm.pc.hardware\n",
    "comp.sys.mac.hardware\n",
    "comp.windows.x\trec.autos\n",
    "rec.motorcycles\n",
    "rec.sport.baseball\n",
    "rec.sport.hockey\t\n",
    "sci.crypt\n",
    "sci.electronics\n",
    "sci.med\n",
    "sci.space\n",
    "misc.forsale\t\n",
    "talk.politics.misc\n",
    "talk.politics.guns\n",
    "talk.politics.mideast\t\n",
    "talk.religion.misc\n",
    "alt.atheism\n",
    "soc.religion.christian`\n",
    "\n",
    "Como você pode ver, alguns pares de grupos podem ser bastante semelhantes, enquanto outros são muito diferentes.\n",
    "\n",
    "Os dados são fornecidos como um conjunto de treinamento designado de tamanho 11314 e conjunto de teste de tamanho 7532. As 20 categorias são representadas em proporções aproximadamente iguais, portanto a precisão de base é cerca de 5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para começar, revise o código abaixo. Isso irá guiá-lo pelos conceitos básicos de carregar os dados do 20 newsgroups, carregar os dados do GloVe, construir a matriz de embeddings de palavras e construir o modelo LSTM.\n",
    "\n",
    "Depois de construirmos o primeiro modelo LSTM, será sua vez de construir um e experimentar com os parâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminares\n",
    "\n",
    "from __future__ import absolute_import, division, print_function  # Compatibilidade Python 2/3\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "seq_length = 30  # Comprimento das sequências de palavras\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixar os dados do 20 newsgroups - já existe um conjunto designado de \"train\" e \"test\"\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(newsgroups_train.data), len(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train = tokenizer.texts_to_sequences(newsgroups_train.data)\n",
    "sequences_test = tokenizer.texts_to_sequences(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Encontrados %s tokens únicos.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(sequences_train, maxlen=seq_length)\n",
    "x_test = pad_sequences(sequences_test, maxlen=seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(np.asarray(newsgroups_train.target))\n",
    "y_test = keras.utils.to_categorical(np.asarray(newsgroups_test.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos os vetores de palavras pré-treinados GloVe. Se você ainda não o fez, por favor baixe-os usando este link:\n",
    "(NOTA: isso iniciará o download de um arquivo de 822MB)\n",
    "\n",
    "http://nlp.stanford.edu/data/glove.6B.zip\n",
    "\n",
    "Em seguida, descompacte o arquivo e preencha seu caminho local para o arquivo na célula de código abaixo.\n",
    "\n",
    "Usaremos o arquivo `glove.6B.100d.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('/Users/lucenator/Work/Data/glove/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Encontrados %s vetores de palavras.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos apenas observar um embedding de palavra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_vec = embeddings_index['dog']\n",
    "dog_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Isso cria uma matriz onde a i-ésima linha fornece o embedding de palavra para a palavra representada pelo inteiro i.\n",
    "## Essencialmente, esses serão os \"pesos\" para a Camada de Embedding\n",
    "## Em vez de aprender os pesos, usaremos esses e \"congelaremos\" a camada\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # palavras não encontradas no índice de embeddings serão todas zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camada LSTM\n",
    "`keras.layers.recurrent.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)`\n",
    "\n",
    "- Estrutura similar à camada `SimpleRNN`\n",
    "- `units` define a dimensão do estado recorrente\n",
    "- `recurrent_...` refere-se aos aspectos do estado recorrente do LSTM\n",
    "- `kernel_...` refere-se às transformações feitas na entrada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dimension = 100  # Esta é a dimensão das palavras que estamos usando do GloVe\n",
    "model = Sequential([\n",
    "    Embedding(len(word_index) + 1,\n",
    "              word_dimension,  \n",
    "              weights=[embedding_matrix],  # Definimos os pesos como os vetores de palavras do GloVe\n",
    "              input_length=seq_length,\n",
    "              trainable=False),  # Ao definir trainable como False, \"congelamos\" os embeddings de palavras.\n",
    "    LSTM(30, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(20, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(learning_rate=.002)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício\n",
    "### Sua Vez\n",
    "- Construa uma rede neural com um SimpleRNN em vez de um LSTM (com outras dimensões e parâmetros iguais). Como o desempenho se compara?\n",
    "- Use o LSTM acima sem os vetores de palavras pré-treinados (inicialize aleatoriamente os pesos e deixe-os serem aprendidos durante o processo de treinamento). Como o desempenho se compara?\n",
    "- Experimente diferentes comprimentos de sequência e dimensões para o estado oculto do LSTM. Você consegue melhorar o modelo?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por favor, forneça seu código aqui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
