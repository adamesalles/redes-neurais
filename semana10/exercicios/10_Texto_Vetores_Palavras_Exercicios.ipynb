{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipgrams em Keras\n",
    "\n",
    "- Nesta aula, implementaremos Skipgrams em `Keras`.\n",
    "\n",
    "#### Carregando e pré-processando dados\n",
    "- Carregar os dados de Alice no País das Maravilhas no Corpus usando o utilitário Keras\n",
    "- `Keras` também possui alguns recursos úteis de pré-processamento de texto!\n",
    "- Dividir o texto em sentenças.\n",
    "- Usar o `Tokenizer` do `Keras` para tokenizar sentenças em palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Básicos\n",
    "from __future__ import absolute_import, division, print_function  # Compatibilidade Python 2/3\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import SVG\n",
    "%matplotlib inline\n",
    "\n",
    "# nltk\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "# keras\n",
    "np.random.seed(13)\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Reshape, Activation\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos usar Alice no País das Maravilhas\n",
    "\n",
    "path = get_file('carrol-alice.txt', origin=\"http://www.gutenberg.org/files/11/11-0.txt\")\n",
    "corpus = open(path).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeiro, dividir o documento em sentenças\n",
    "corpus = corpus[corpus.index('\\n\\n')+2:]  # remover cabeçalho.\n",
    "sentences = sent_tokenize(corpus)\n",
    "\n",
    "# Tokenizar usando Keras\n",
    "base_filter='!\"#$%&()*+,-./:;`<=>?@[\\\\]^_{|}~\\t\\n' + \"'\"\n",
    "tokenizer = Tokenizer(filters=base_filter)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Converter sentenças tokenizadas para formato de sequência\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "\n",
    "print(len(sequences), tokenizer.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para entender o que está acontecendo;\n",
    "\n",
    "print(sentences[324])  # esta é uma sentença\n",
    "print(sequences[324])  # esta é a mesma sentença onde as palavras são codificadas como números.\n",
    "print(list(tokenizer.word_index[word.lower().replace('.', '')] \n",
    "           for word in sentences[324].split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skipgrams: Gerando Rótulos de Entrada e Saída\n",
    "- Agora que temos sentenças e tokenização de palavras, estamos em uma boa posição para criar nosso conjunto de treinamento para skipgrams.\n",
    "- Agora precisamos gerar nosso `X_train` e `y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeiro, vamos ver como a função skipgrams do Keras funciona.\n",
    "\n",
    "couples, labels = skipgrams(sequences[324], len(tokenizer.word_index) + 1,\n",
    "    window_size=2, negative_samples=0, shuffle=True,\n",
    "    categorical=False, sampling_table=None)\n",
    "\n",
    "index_2_word = {val: key for key, val in tokenizer.word_index.items()}\n",
    "\n",
    "for w1, w2 in couples:\n",
    "    if w1 == 13:\n",
    "        print(index_2_word[w1], index_2_word[w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para gerar as entradas e saídas para todas as janelas\n",
    "\n",
    "# Tamanho do vocabulário\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# Dimensão para reduzir\n",
    "dim = 100\n",
    "window_size = 2\n",
    "\n",
    "\n",
    "def generate_data(sequences, window_size, vocab_size):\n",
    "    for seq in sequences:\n",
    "        X, y = [], []\n",
    "        couples, _ = skipgrams(\n",
    "            seq, vocab_size,\n",
    "            window_size=window_size, negative_samples=0, shuffle=True,\n",
    "            categorical=False, sampling_table=None)\n",
    "        if not couples:\n",
    "            continue\n",
    "        for in_word, out_word in couples:\n",
    "            X.append(in_word)\n",
    "            y.append(keras.utils.to_categorical(out_word, vocab_size))\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        X = X.reshape(len(X), 1)\n",
    "        y = y.reshape(len(X), vocab_size)\n",
    "        yield X, y\n",
    "        \n",
    "data_generator = generate_data(sequences, window_size, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipgrams: Criando o Modelo\n",
    "- Por último, criamos a rede (rasa)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o modelo Keras e visualizá-lo \n",
    "skipgram = Sequential()\n",
    "skipgram.add(Embedding(input_dim=vocab_size, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "skipgram.add(Reshape((dim,)))\n",
    "skipgram.add(Dense(input_dim=dim, units=vocab_size, activation='softmax'))\n",
    "# SVG(model_to_dot(skipgram, show_shapes=True).create(prog='dot', format='svg'))\n",
    "# Nota: model_to_dot foi descontinuado, use plot_model se necessário visualizar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipgrams: Compilando e Treinando\n",
    "- Hora de compilar e treinar\n",
    "- Usamos entropia cruzada, perda comum para classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar o Modelo Keras\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "sgd = SGD(learning_rate=1e-4, decay=1e-6, momentum=0.9)\n",
    "\n",
    "skipgram.compile(loss='categorical_crossentropy', optimizer=\"adadelta\")\n",
    "\n",
    "# Treinar os Skipgrams\n",
    "for iteration in range(10):\n",
    "    loss = 0\n",
    "    for x, y in generate_data(sequences, window_size, vocab_size):\n",
    "        loss += skipgram.train_on_batch(x, y)\n",
    "    print('iteração {}, perda é {}'.format(iteration, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipgrams: Observando os vetores\n",
    "\n",
    "Para obter word_vectors agora, observamos os pesos da primeira camada.\n",
    "\n",
    "Vamos também escrever funções que nos dão a similaridade de duas palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = skipgram.get_weights()[0]\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "def get_dist(w1, w2):\n",
    "    i1, i2 = tokenizer.word_index[w1], tokenizer.word_index[w2]\n",
    "    v1, v2 = word_vectors[i1], word_vectors[i2]\n",
    "    return cosine(v1, v2)\n",
    "\n",
    "def get_similarity(w1, w2):\n",
    "    return 1-get_dist(w1, w2)\n",
    "\n",
    "def get_most_similar(w1, n=10):\n",
    "    sims = {word: get_similarity(w1, word) \n",
    "            for word in tokenizer.word_index.keys()\n",
    "            if word != w1}\n",
    "    sims = pd.Series(sims)\n",
    "    sims.sort_values(inplace=True, ascending=False)\n",
    "    return sims.iloc[:n]\n",
    "\n",
    "\n",
    "print(get_similarity('king', 'queen'))\n",
    "print('')\n",
    "print(get_most_similar('queen'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sua vez -- Modifique o código acima para criar um Modelo CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
