{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando RNNs para classificar sentimento em dados do IMDB\n",
    "Neste exercício, treinaremos uma RNN \"vanilla\" para prever o sentimento em avaliações do IMDB. Nossos dados consistem em 25000 sequências de treinamento e 25000 sequências de teste. O resultado é binário (positivo/negativo) e ambos os resultados são igualmente representados tanto no conjunto de treinamento quanto no conjunto de teste.\n",
    "\n",
    "O Keras fornece uma interface conveniente para carregar os dados e codificar imediatamente as palavras em inteiros (com base nas palavras mais comuns). Isso nos poupará muito do trabalho pesado que geralmente está envolvido ao trabalhar com texto bruto.\n",
    "\n",
    "Vamos percorrer a preparação dos dados e a construção de um modelo RNN. Então será sua vez de construir seus próprios modelos (e preparar os dados como achar melhor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000  # Usado no carregamento dos dados, seleciona as palavras mais comuns (max_features)\n",
    "maxlen = 30  # comprimento máximo de uma sequência - truncar após isso\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Carregar os dados. A função tokeniza automaticamente o texto em inteiros distintos\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'sequências de treino')\n",
    "print(len(x_test), 'sequências de teste')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isso preenche (ou trunca) as sequências para que tenham o comprimento máximo\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('forma de x_train:', x_train.shape)\n",
    "print('forma de x_test:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[123,:]  # Aqui está como uma sequência de exemplo se parece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camadas Keras para RNNs (Vanilla)\n",
    "\n",
    "Neste exercício, não usaremos vetores de palavras pré-treinados. Em vez disso, aprenderemos um embedding como parte da Rede Neural. Isso é representado pela Camada de Embedding abaixo.\n",
    "\n",
    "### Camada de Embedding\n",
    "`keras.layers.embeddings.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)`\n",
    "\n",
    "- Esta camada mapeia cada inteiro em um vetor de palavra (denso) distinto de comprimento `output_dim`.\n",
    "- Pode-se pensar nisso como aprender um embedding de vetor de palavra \"em tempo real\" em vez de usar um mapeamento existente (como GloVe)\n",
    "- O `input_dim` deve ser o tamanho do vocabulário.\n",
    "- O `input_length` especifica o comprimento das sequências que a rede espera.\n",
    "\n",
    "### Camada SimpleRNN\n",
    "`keras.layers.recurrent.SimpleRNN(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)`\n",
    "\n",
    "- Esta é a RNN básica, onde a saída também é alimentada de volta como o \"estado oculto\" para a próxima iteração.\n",
    "- O parâmetro `units` fornece a dimensionalidade da saída (e, portanto, do estado oculto). Note que tipicamente haverá outra camada após a RNN mapeando a saída (RNN) para a saída da rede. Então devemos pensar neste valor como a dimensionalidade desejada do estado oculto e não necessariamente a saída desejada da rede.\n",
    "- Lembre-se de que existem dois conjuntos de pesos, um para a fase \"recorrente\" e outro para a fase \"kernel\". Eles podem ser configurados separadamente em termos de sua inicialização, regularização, etc.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vamos construir uma RNN\n",
    "\n",
    "rnn_hidden_dim = 5\n",
    "word_embedding_dim = 50\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(max_features, word_embedding_dim))  # Esta camada pega cada inteiro na sequência e o incorpora em um vetor de 50 dimensões\n",
    "model_rnn.add(SimpleRNN(rnn_hidden_dim,\n",
    "                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n",
    "                    recurrent_initializer=initializers.Identity(gain=1.0),\n",
    "                    activation='relu',\n",
    "                    input_shape=x_train.shape[1:]))\n",
    "\n",
    "model_rnn.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note que a maioria dos parâmetros vem da camada de embedding\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(learning_rate=.0001)\n",
    "\n",
    "model_rnn.compile(loss='binary_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = model_rnn.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Pontuação no teste:', score)\n",
    "print('Precisão no teste:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício\n",
    "### Sua Vez\n",
    "\n",
    "Agora faça você mesmo:\n",
    "- Prepare os dados para usar sequências de comprimento 80 em vez de comprimento 30. Isso melhorou o desempenho?\n",
    "- Tente diferentes valores de \"max_features\". Você consegue melhorar o desempenho?\n",
    "- Tente tamanhos menores e maiores da dimensão oculta da RNN. Como isso afeta o desempenho do modelo? Como afeta o tempo de execução?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escreva seu código aqui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
